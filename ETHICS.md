# Ethics Statement

## Purpose of the Project

BiasLab is a research and auditing framework designed to study **narrative divergence**
and **normative framing differences** in large language models (LLMs).
Its primary goal is to support transparency, reproducibility, and critical inquiry
into how AI systems frame socially sensitive, normative, and value-laden topics.

BiasLab is **not** intended to rank models, declare moral superiority, or promote
any political, ideological, or commercial agenda.

---

## Research Scope and Limitations

BiasLab analyzes **model outputs**, not intentions, beliefs, or internal states.
All findings should be interpreted as properties of *generated text under specific prompts*,
not as definitive statements about a modelâ€™s values or alignment.

Key limitations include:
- dependence on prompt selection,
- sensitivity to temperature and decoding strategies,
- temporal instability of proprietary models,
- absence of access to training data or internal representations.

BiasLab explicitly avoids claims of absolute objectivity.

---

## Treatment of Sensitive Topics

Some research questions involve socially sensitive domains (e.g. race, power,
responsibility, inequality, harm).
These topics are included **solely for analytical purposes**, to study framing
patterns and response asymmetries.

BiasLab does **not** endorse, promote, or legitimize harmful stereotypes.
The framework is designed to *surface differences*, not to normalize or amplify them.

---

## Non-Misuse Policy

BiasLab must not be used for:
- harassment, discrimination, or targeting of individuals or groups,
- automated scoring or blacklisting of models without contextual analysis,
- marketing claims implying moral or ethical certification of AI systems,
- political persuasion or profiling.

Any downstream use of BiasLab outputs remains the responsibility of the user.

---

## Reproducibility and Transparency

BiasLab prioritizes:
- open-source code,
- transparent metrics,
- reproducible pipelines,
- explicit documentation of assumptions.

All methodological choices are intended to be inspectable, challengeable,
and improvable by the research community.

---

## Human Oversight

BiasLab is a **decision-support and research tool**, not an automated decision-maker.
Interpretation of results requires human judgment, contextual knowledge,
and ethical reflection.

No automated conclusions should be drawn without expert review.

---

## Licensing and Open Research Commitment

BiasLab is released under the Apache 2.0 License.
All research artifacts are intended for open scientific use,
consistent with the principles of open science and public-interest technology.

---

## Contact

Ethical concerns, questions, or requests for clarification may be directed to:

Tomasz Trojanowski  
Data Never Lies  
tomas@dataneverlies.org
